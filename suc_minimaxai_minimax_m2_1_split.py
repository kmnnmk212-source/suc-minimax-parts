# -*- coding: utf-8 -*-
"""suc_MiniMaxAI_MiniMax-M2.1_split.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pckyG7JwzmLLc_1JOvaw-Ix8GWbYkojW
"""



!wget https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf

!wget https://github.com/ggml-org/llama.cpp/releases/download/b7622/llama-b7622-bin-ubuntu-x64.tar.gz

!tar -xzf llama-b7622-bin-ubuntu-x64.tar.gz

!/content/llama-b7622/llama-clÿß

!/content/llama-b7622/llama-cli -h

!/content/llama-b7622/llama-gguf-split -h

!/content/llama-b7622/llama-gguf-split --split-max-size 10G /content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf /content/MiniMaxAI_MiniMax-M2.1-IQ1_S







"""# Task
Upload the split GGUF model files, generated from "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf" and prefixed with "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S", to the Hugging Face repository "asdgad/minimax".

## Install Hugging Face Hub Library

### Subtask:
Install the `huggingface_hub` Python library, which is necessary for interacting with the Hugging Face platform to upload your model files.

**Reasoning**:
To install the `huggingface_hub` library, I will use the `pip install` command in a code cell.
"""

pip install huggingface_hub

"""## Authenticate with Hugging Face

### Subtask:
Authenticate your Colab environment with Hugging Face using your API token. This will allow the `huggingface_hub` library to upload files to your account.

**Reasoning**:
To authenticate with Hugging Face, I need to import the `notebook_login` function and call it, which will prompt for the API token.
"""

from huggingface_hub import notebook_login

notebook_login()

"""## List Split Model Files

### Subtask:
List the files generated by the `llama-gguf-split` command to confirm their names and locations. These will likely be in the `/content/` directory and follow a pattern like `MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-N.gguf`.

**Reasoning**:
To list the split GGUF model files, I will use the `ls` command with a wildcard to match the specified prefix and file extension in the `/content/` directory.
"""

import glob

file_pattern = "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S*.gguf"
split_files = glob.glob(file_pattern)

if split_files:
    for file_path in split_files:
        print(file_path)
else:
    print(f"No files found matching the pattern: {file_pattern}")

"""### Re-attempt Splitting the GGUF Model

The previous attempt to split the GGUF model using `llama-gguf-split` did not generate the expected multiple files. The listing command in the previous step confirmed that only the original, unsplit file (`/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf`) was present.

This indicates that the splitting operation might not have completed successfully or the output was not in the expected format. To resolve this, we will re-run the `llama-gguf-split` command to ensure the model is correctly divided into smaller GGUF files (each with a maximum size of 10GB). This is crucial before we can proceed with uploading the split components to Hugging Face.

**Reasoning**:
To correctly split the GGUF model into smaller files for easier uploading, I will re-execute the `llama-gguf-split` command with the specified `--split-max-size 10G` parameter, using the previously downloaded model as input and a new prefix for the output files.
"""

!/content/llama-b7622/llama-gguf-split --split-max-size 10G /content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf /content/MiniMaxAI_MiniMax-M2.1-IQ1_S

"""**Reasoning**:
To confirm that the `llama-gguf-split` command successfully created multiple split GGUF files, I will list the files in the `/content/` directory that match the specified pattern.


"""

import glob

file_pattern = "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S*.gguf"
split_files = glob.glob(file_pattern)

if split_files:
    for file_path in split_files:
        print(file_path)
else:
    print(f"No files found matching the pattern: {file_pattern}")

"""**Reasoning**:
Now that the GGUF model has been successfully split into multiple files, I will upload these files to the specified Hugging Face repository. I will use the `HfApi` from `huggingface_hub` to upload each file found by the `glob` pattern.


"""

from huggingface_hub import HfApi
import glob

api = HfApi()
repo_id = "asdgad/minimax"
file_pattern = "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-*-of-*.gguf" # Only target the split files
split_files = glob.glob(file_pattern)

if split_files:
    print(f"Uploading {len(split_files)} split GGUF files to {repo_id}...")
    for file_path in split_files:
        file_name = file_path.split('/')[-1]
        print(f"Uploading {file_name}...")
        api.upload_file(
            path_or_fileobj=file_path,
            path_in_repo=file_name,
            repo_id=repo_id,
            repo_type="model",
        )
    print("All split GGUF files uploaded successfully.")
else:
    print(f"No split GGUF files found matching the pattern: {file_pattern}. Please ensure the splitting step was successful.")

"""## Final Task

### Subtask:
Confirm that all split model files have been successfully uploaded to the Hugging Face repository and provide the link to the uploaded model.

## Summary:

### Q&A
*   **Confirmation of split model file upload:** All 5 split GGUF model files were successfully uploaded to the Hugging Face repository.
*   **Link to the uploaded model:** The uploaded model files can be found at `https://huggingface.co/asdgad/minimax/tree/main`.

### Data Analysis Key Findings
*   The `huggingface_hub` library was confirmed to be already installed in the environment.
*   Hugging Face authentication was successfully initiated, awaiting user token input.
*   An initial check revealed that the GGUF model was not split, with only the original file present.
*   The `llama-gguf-split` command was re-executed, successfully splitting the `/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf` model into 5 smaller GGUF files, each with a maximum size of 10GB.
*   All 5 newly generated split GGUF files (e.g., `MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf`) were successfully uploaded to the Hugging Face repository "asdgad/minimax".

### Insights or Next Steps
*   Users can verify the uploaded split GGUF files directly on the Hugging Face model page.
*   Ensure the integrity and functionality of the split model files on Hugging Face for future use or sharing.
"""

!/content/llama-b7622/llama-cli -m /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf

