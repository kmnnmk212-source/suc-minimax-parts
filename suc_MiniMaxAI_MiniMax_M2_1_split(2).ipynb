{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhOA0D0kO1QG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LHpYSNHPIEq",
        "outputId": "77f16ebb-3627-46e8-ae12-ff523d91034e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-04 18:22:46--  https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/694edaab61707ccb93a7e291/727abcd1ab070293c4574cd225c9de6eb7588f2760dd1ff5a3a44182efdd8b91?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260104T182246Z&X-Amz-Expires=3600&X-Amz-Signature=08dbeea4a8bc10f081ef14bbdd3abf7c3dc8a3837bdf504ca01e308fb63e6f22&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf%3B+filename%3D%22MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf%22%3B&x-id=GetObject&Expires=1767554566&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzU1NDU2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTRlZGFhYjYxNzA3Y2NiOTNhN2UyOTEvNzI3YWJjZDFhYjA3MDI5M2M0NTc0Y2QyMjVjOWRlNmViNzU4OGYyNzYwZGQxZmY1YTNhNDQxODJlZmRkOGI5MSoifV19&Signature=NQmMO3cVeb35ibIv6qsL5yFohBGfPjM2es56olo%7E47fiijULUK2Hwg4h-R1LS1f84nQc-SQj-4xKuGCqGYG9278HxFiEoJEIO07VRBrPSgKE998xcZyXdFaUXTaFIFKNuy1d07yMbt6ZZVMnbai8ciQSfrwJHKCbKwYgaLmWjuWUES8ZcgsBHOcMA0Hp4uracVKH0IgQI5ATIlGuHPYfjh3ddXMBj7uh9R117TuApVt5bbHSR86Nh1vYiLLcukfQZYHi8v44zknwXX71thmFYFdsaPe-JsMlU3yGhK4fuWdNqFgw-Ue8X5EkluQCWDI2jiY8IfTium0V6ZhpuTGnwg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2026-01-04 18:22:46--  https://cas-bridge.xethub.hf.co/xet-bridge-us/694edaab61707ccb93a7e291/727abcd1ab070293c4574cd225c9de6eb7588f2760dd1ff5a3a44182efdd8b91?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260104T182246Z&X-Amz-Expires=3600&X-Amz-Signature=08dbeea4a8bc10f081ef14bbdd3abf7c3dc8a3837bdf504ca01e308fb63e6f22&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf%3B+filename%3D%22MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf%22%3B&x-id=GetObject&Expires=1767554566&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzU1NDU2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTRlZGFhYjYxNzA3Y2NiOTNhN2UyOTEvNzI3YWJjZDFhYjA3MDI5M2M0NTc0Y2QyMjVjOWRlNmViNzU4OGYyNzYwZGQxZmY1YTNhNDQxODJlZmRkOGI5MSoifV19&Signature=NQmMO3cVeb35ibIv6qsL5yFohBGfPjM2es56olo%7E47fiijULUK2Hwg4h-R1LS1f84nQc-SQj-4xKuGCqGYG9278HxFiEoJEIO07VRBrPSgKE998xcZyXdFaUXTaFIFKNuy1d07yMbt6ZZVMnbai8ciQSfrwJHKCbKwYgaLmWjuWUES8ZcgsBHOcMA0Hp4uracVKH0IgQI5ATIlGuHPYfjh3ddXMBj7uh9R117TuApVt5bbHSR86Nh1vYiLLcukfQZYHi8v44zknwXX71thmFYFdsaPe-JsMlU3yGhK4fuWdNqFgw-Ue8X5EkluQCWDI2jiY8IfTium0V6ZhpuTGnwg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.63, 18.238.217.126, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47009105696 (44G)\n",
            "Saving to: \u2018MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\u2019\n",
            "\n",
            "MiniMaxAI_MiniMax-M 100%[===================>]  43.78G  45.6MB/s    in 15m 45s \n",
            "\n",
            "2026-01-04 18:38:32 (47.4 MB/s) - \u2018MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\u2019 saved [47009105696/47009105696]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b7622/llama-b7622-bin-ubuntu-x64.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_-LEb4rPWIz",
        "outputId": "3b9b2868-e6c7-4e96-f659-57af2c3bb76c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-04 18:20:49--  https://github.com/ggml-org/llama.cpp/releases/download/b7622/llama-b7622-bin-ubuntu-x64.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/26583b38-d6e7-4df5-834e-91cf364dcb7a?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-04T18%3A56%3A49Z&rscd=attachment%3B+filename%3Dllama-b7622-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-04T17%3A56%3A19Z&ske=2026-01-04T18%3A56%3A49Z&sks=b&skv=2018-11-09&sig=w59MUO4wni%2BYiz8dSFr1jwALbVXP8tmUdNFwGV9rHaQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzU1MjY0OSwibmJmIjoxNzY3NTUwODQ5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.CazPwYpiobs9x5yh9DTenJfk0ZdXAFT2Mw3lKjrvR5A&response-content-disposition=attachment%3B%20filename%3Dllama-b7622-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-01-04 18:20:49--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/26583b38-d6e7-4df5-834e-91cf364dcb7a?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-04T18%3A56%3A49Z&rscd=attachment%3B+filename%3Dllama-b7622-bin-ubuntu-x64.tar.gz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-04T17%3A56%3A19Z&ske=2026-01-04T18%3A56%3A49Z&sks=b&skv=2018-11-09&sig=w59MUO4wni%2BYiz8dSFr1jwALbVXP8tmUdNFwGV9rHaQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzU1MjY0OSwibmJmIjoxNzY3NTUwODQ5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.CazPwYpiobs9x5yh9DTenJfk0ZdXAFT2Mw3lKjrvR5A&response-content-disposition=attachment%3B%20filename%3Dllama-b7622-bin-ubuntu-x64.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21548302 (21M) [application/octet-stream]\n",
            "Saving to: \u2018llama-b7622-bin-ubuntu-x64.tar.gz\u2019\n",
            "\n",
            "llama-b7622-bin-ubu 100%[===================>]  20.55M  99.2MB/s    in 0.2s    \n",
            "\n",
            "2026-01-04 18:20:49 (99.2 MB/s) - \u2018llama-b7622-bin-ubuntu-x64.tar.gz\u2019 saved [21548302/21548302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c39b80b5"
      },
      "source": [
        "!tar -xzf llama-b7622-bin-ubuntu-x64.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-b7622/llama-cl\u0627"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElAy3OYZPYEh",
        "outputId": "1093c22f-6f95-410c-e794-1b05325cf0a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7622/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7622/libggml-cpu-haswell.so\n",
            "error: invalid argument: -\u0627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-b7622/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLvlgeASPnWK",
        "outputId": "24332922-9d33-4d17-8859-0676de7a6045"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7622/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7622/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "-cl,   --cache-list                     show list of models in cache\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of CPU threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "-fa,   --flash-attn [on|off|auto]       set Flash Attention use ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--perf, --no-perf                       whether to enable internal libllama performance timings (default:\n",
            "                                        false)\n",
            "                                        (env: LLAMA_ARG_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape, --no-escape            whether to process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "                                        (default: true)\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-kvo,  --kv-offload, -nkvo, --no-kv-offload\n",
            "                                        whether to enable KV cache offloading (default: enabled)\n",
            "                                        (env: LLAMA_ARG_KV_OFFLOAD)\n",
            "--repack, -nr, --no-repack              whether to enable weight repacking (default: enabled)\n",
            "                                        (env: LLAMA_ARG_REPACK)\n",
            "--no-host                               bypass host buffer allowing extra buffers to be used\n",
            "                                        (env: LLAMA_ARG_NO_HOST)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers (host:port)\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--mmap, --no-mmap                       whether to memory-map model (if disabled, slower load but may reduce\n",
            "                                        pageouts if not using mlock) (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "-ot,   --override-tensor <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "                                        (env: LLAMA_ARG_OVERRIDE_TENSOR)\n",
            "-cmoe, --cpu-moe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "-ncmoe, --n-cpu-moe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   max. number of layers to store in VRAM, either an exact number,\n",
            "                                        'auto', or 'all' (default: auto)\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "-fit,  --fit [on|off]                   whether to adjust unset arguments to fit in device memory ('on' or\n",
            "                                        'off', default: 'on')\n",
            "                                        (env: LLAMA_ARG_FIT)\n",
            "-fitt, --fit-target MiB                 target margin per device for --fit option, default: 1024\n",
            "                                        (env: LLAMA_ARG_FIT_TARGET)\n",
            "-fitc, --fit-ctx N                      minimum ctx size that can be set by --fit option, default: 4096\n",
            "                                        (env: LLAMA_ARG_FIT_CTX)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE,...        advanced option to override model metadata by key. to specify multiple\n",
            "                                        overrides, either use comma-separated or repeat this argument.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false,tokenizer.ggml.add_eos_token=bool:false\n",
            "--op-offload, --no-op-offload           whether to offload host tensor operations to device (default: true)\n",
            "--lora FNAME                            path to LoRA adapter (use comma-separated values to load multiple\n",
            "                                        adapters)\n",
            "--lora-scaled FNAME:SCALE,...           path to LoRA adapter with user defined scaling (format:\n",
            "                                        FNAME:SCALE,...)\n",
            "                                        note: use comma-separated values\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: use comma-separated values to add multiple control vectors\n",
            "--control-vector-scaled FNAME:SCALE,...\n",
            "                                        add a control vector with user defined scaling SCALE\n",
            "                                        note: use comma-separated values (format: FNAME:SCALE,...)\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path to load\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-dr,   --docker-repo [<repo>/]<model>[:quant]\n",
            "                                        Docker Hub model repository. repo is optional, default to ai/. quant\n",
            "                                        is optional, default to :latest.\n",
            "                                        example: gemma3\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_DOCKER_REPO)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "                                        (env: LLAMA_LOG_FILE)\n",
            "--log-colors [on|off|auto]              Set colored logging ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored. Values:\n",
            "                                         - 0: generic output\n",
            "                                         - 1: error\n",
            "                                         - 2: warning\n",
            "                                         - 3: info\n",
            "                                         - 4: debug\n",
            "                                        (default: 1)\n",
            "                                        \n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampler-seq, --sampling-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "                                        (env: LLAMA_ARG_TOP_K)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--display-prompt, --no-display-prompt   whether to print prompt at generation (default: true)\n",
            "-co,   --color [on|off|auto]            Colorize output to distinguish prompt and user input from generations\n",
            "                                        ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "--ctx-checkpoints, --swa-checkpoints N\n",
            "                                        max number of context checkpoints to create per slot (default:\n",
            "                                        8)[(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)\n",
            "                                        (env: LLAMA_ARG_CTX_CHECKPOINTS)\n",
            "-cram, --cache-ram N                    set the maximum cache size in MiB (default: 8192, -1 - no limit, 0 -\n",
            "                                        disable)[(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/16391)\n",
            "                                        (env: LLAMA_ARG_CACHE_RAM)\n",
            "--context-shift, --no-context-shift     whether to use context shift on infinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "--show-timings, --no-show-timings       whether to show timing information after each response (default: true)\n",
            "                                        (env: LLAMA_ARG_SHOW_TIMINGS)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation, -no-cnv, --no-conversation\n",
            "                                        whether to run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--warmup, --no-warmup                   whether to perform warmup with an empty run (default: enabled)\n",
            "-mm,   --mmproj FILE                    path to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        note: if -hf is used, this argument can be omitted\n",
            "                                        (env: LLAMA_ARG_MMPROJ)\n",
            "-mmu,  --mmproj-url URL                 URL to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        (env: LLAMA_ARG_MMPROJ_URL)\n",
            "--mmproj-auto, --no-mmproj, --no-mmproj-auto\n",
            "                                        whether to use multimodal projector file (if available), useful when\n",
            "                                        using -hf (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_AUTO)\n",
            "--mmproj-offload, --no-mmproj-offload   whether to enable GPU offloading for multimodal projector (default:\n",
            "                                        enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_OFFLOAD)\n",
            "--image, --audio FILE                   path to an image or audio file. use with multimodal models, use\n",
            "                                        comma-separated values for multiple files\n",
            "--image-min-tokens N                    minimum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MIN_TOKENS)\n",
            "--image-max-tokens N                    maximum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MAX_TOKENS)\n",
            "-otd,  --override-tensor-draft <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type for draft model\n",
            "-cmoed, --cpu-moe-draft                 keep all Mixture of Experts (MoE) weights in the CPU for the draft\n",
            "                                        model\n",
            "                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)\n",
            "-ncmoed, --n-cpu-moe-draft N            keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU for the draft model\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)\n",
            "--chat-template-kwargs STRING           sets additional params for the json template parser\n",
            "                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)\n",
            "--jinja, --no-jinja                     whether to use jinja template engine for chat (default: enabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content`\n",
            "                                        - deepseek-legacy: keeps `<think>` tags in `message.content` while\n",
            "                                        also populating `message.reasoning_content`\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, solar-open, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, solar-open, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "--draft, --draft-n, --draft-max N       number of tokens to draft for speculative decoding (default: 16)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MAX)\n",
            "--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding\n",
            "                                        (default: 0)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MIN)\n",
            "--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)\n",
            "                                        (env: LLAMA_ARG_DRAFT_P_MIN)\n",
            "-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded\n",
            "                                        from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)\n",
            "-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model\n",
            "                                        (none = don't offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "-ngld, --gpu-layers-draft, --n-gpu-layers-draft N\n",
            "                                        max. number of draft model layers to store in VRAM, either an exact\n",
            "                                        number, 'auto', or 'all' (default: auto)\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)\n",
            "-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_DRAFT)\n",
            "--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main\n",
            "                                        model are not compatible\n",
            "--gpt-oss-20b-default                   use gpt-oss-20b (note: can download weights from the internet)\n",
            "--gpt-oss-120b-default                  use gpt-oss-120b (note: can download weights from the internet)\n",
            "--vision-gemma-4b-default               use Gemma 3 4B QAT (note: can download weights from the internet)\n",
            "--vision-gemma-12b-default              use Gemma 3 12B QAT (note: can download weights from the internet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-b7622/llama-gguf-split -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD9x1yeOPtPR",
        "outputId": "d9d93528-bd4f-4149-98f5-a106594c2816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: bad arguments\n",
            "\n",
            "usage: /content/llama-b7622/llama-gguf-split [options] GGUF_IN GGUF_OUT\n",
            "\n",
            "Apply a GGUF operation on IN to OUT.\n",
            "options:\n",
            "  -h, --help              show this help message and exit\n",
            "  --version               show version and build info\n",
            "  --split                 split GGUF to multiple GGUF (enabled by default)\n",
            "  --merge                 merge multiple GGUF to a single GGUF\n",
            "  --split-max-tensors     max tensors in each split (default: 128)\n",
            "  --split-max-size N(M|G) max size per split\n",
            "  --no-tensor-first-split do not add tensors to the first split (disabled by default)\n",
            "  --dry-run               only print out a split plan and exit, without writing any new files\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-b7622/llama-gguf-split --split-max-size 10G /content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf /content/MiniMaxAI_MiniMax-M2.1-IQ1_S"
      ],
      "metadata": {
        "id": "tE1y2LQ-PxZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Gu2G7yoRDNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQasxlmMRDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBEXTd_PRDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a08664f7"
      },
      "source": [
        "# Task\n",
        "Upload the split GGUF model files, generated from \"/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\" and prefixed with \"/content/MiniMaxAI_MiniMax-M2.1-IQ1_S\", to the Hugging Face repository \"asdgad/minimax\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f237c7be"
      },
      "source": [
        "## Install Hugging Face Hub Library\n",
        "\n",
        "### Subtask:\n",
        "Install the `huggingface_hub` Python library, which is necessary for interacting with the Hugging Face platform to upload your model files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ab8e54c"
      },
      "source": [
        "**Reasoning**:\n",
        "To install the `huggingface_hub` library, I will use the `pip install` command in a code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "250fca45",
        "outputId": "a639e068-d77c-4ca9-ce7e-705c2843ff4f"
      },
      "source": [
        "pip install huggingface_hub"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839a65b7"
      },
      "source": [
        "## Authenticate with Hugging Face\n",
        "\n",
        "### Subtask:\n",
        "Authenticate your Colab environment with Hugging Face using your API token. This will allow the `huggingface_hub` library to upload files to your account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b321bed7"
      },
      "source": [
        "**Reasoning**:\n",
        "To authenticate with Hugging Face, I need to import the `notebook_login` function and call it, which will prompt for the API token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "ada958569f9341cea48d03558f4163b9",
            "85c23bf501e745c1845a2f2752af3918",
            "e4d208c115294b569925863bbb8806c8",
            "9187a1d90d3246cf97b4654be2cc827b",
            "2fa312a71ac846bdadd364953bfd2e65",
            "da8afb3b973a45d4b67c3691a79908fe",
            "56d8f38dbb0e484398879c85edd6399f",
            "6fb46291dcc942a580942e665c1be740",
            "4608c117742b47acbafb7236135c8425",
            "640743a2d1894152a82af02bcb32621b",
            "34bc7c4391274a25b3320d7eabeb9859",
            "3e2f790ba5b7455eb1b93019c744b7ef",
            "16b2471bf93c41abb9b7438b9459fc54",
            "43fce8eba8f04ba3b256bc2ded231ddc",
            "2f7ef399536d4036b6ca0d808917ec32",
            "a0acd129620f424f8e01225fcc95318e",
            "6f7075ababf044c48d916bea37922466",
            "c50171091e5c4d7b9aea8af8b72f011a",
            "64df998e5d52453ba4393e3b8d711c3d",
            "9ede6adf1e154c7a96a4d6548909885e",
            "5d9618a5dae64e2890763aeecadd659b",
            "ab904913b2804787bcbfc845ab786018",
            "17b0834affc045778aa0c60fe9b1557f"
          ]
        },
        "id": "01a11b09",
        "outputId": "57ebf3c1-93c8-4aaa-e23c-4a07b87c3ac1"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ada958569f9341cea48d03558f4163b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cd0d34d"
      },
      "source": [
        "## List Split Model Files\n",
        "\n",
        "### Subtask:\n",
        "List the files generated by the `llama-gguf-split` command to confirm their names and locations. These will likely be in the `/content/` directory and follow a pattern like `MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-N.gguf`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6094c0"
      },
      "source": [
        "**Reasoning**:\n",
        "To list the split GGUF model files, I will use the `ls` command with a wildcard to match the specified prefix and file extension in the `/content/` directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00da23c",
        "outputId": "79e3ee24-4dea-4a3a-feca-fe3b801b264d"
      },
      "source": [
        "import glob\n",
        "\n",
        "file_pattern = \"/content/MiniMaxAI_MiniMax-M2.1-IQ1_S*.gguf\"\n",
        "split_files = glob.glob(file_pattern)\n",
        "\n",
        "if split_files:\n",
        "    for file_path in split_files:\n",
        "        print(file_path)\n",
        "else:\n",
        "    print(f\"No files found matching the pattern: {file_pattern}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29a04776"
      },
      "source": [
        "### Re-attempt Splitting the GGUF Model\n",
        "\n",
        "The previous attempt to split the GGUF model using `llama-gguf-split` did not generate the expected multiple files. The listing command in the previous step confirmed that only the original, unsplit file (`/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf`) was present.\n",
        "\n",
        "This indicates that the splitting operation might not have completed successfully or the output was not in the expected format. To resolve this, we will re-run the `llama-gguf-split` command to ensure the model is correctly divided into smaller GGUF files (each with a maximum size of 10GB). This is crucial before we can proceed with uploading the split components to Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e297b5d1"
      },
      "source": [
        "**Reasoning**:\n",
        "To correctly split the GGUF model into smaller files for easier uploading, I will re-execute the `llama-gguf-split` command with the specified `--split-max-size 10G` parameter, using the previously downloaded model as input and a new prefix for the output files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51189d19",
        "outputId": "c98dfe21-e7f3-493e-87eb-c51bd0a0d75e"
      },
      "source": [
        "!/content/llama-b7622/llama-gguf-split --split-max-size 10G /content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf /content/MiniMaxAI_MiniMax-M2.1-IQ1_S"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_split: 5\n",
            "split 00001: n_tensors = 167, total_size = 9990M\n",
            "split 00002: n_tensors = 173, total_size = 9949M\n",
            "split 00003: n_tensors = 179, total_size = 9964M\n",
            "split 00004: n_tensors = 181, total_size = 9967M\n",
            "split 00005: n_tensors = 109, total_size = 7136M\n",
            "Writing file /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf ... done\n",
            "Writing file /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00002-of-00005.gguf ... done\n",
            "Writing file /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00003-of-00005.gguf ... done\n",
            "Writing file /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00004-of-00005.gguf ... done\n",
            "Writing file /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00005-of-00005.gguf ... done\n",
            "gguf_split: 5 gguf split written with a total of 809 tensors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62b505d1"
      },
      "source": [
        "**Reasoning**:\n",
        "To confirm that the `llama-gguf-split` command successfully created multiple split GGUF files, I will list the files in the `/content/` directory that match the specified pattern.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae2fe32",
        "outputId": "431a26c1-be01-4081-8eca-ab3c0cfba994"
      },
      "source": [
        "import glob\n",
        "\n",
        "file_pattern = \"/content/MiniMaxAI_MiniMax-M2.1-IQ1_S*.gguf\"\n",
        "split_files = glob.glob(file_pattern)\n",
        "\n",
        "if split_files:\n",
        "    for file_path in split_files:\n",
        "        print(file_path)\n",
        "else:\n",
        "    print(f\"No files found matching the pattern: {file_pattern}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00005-of-00005.gguf\n",
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00002-of-00005.gguf\n",
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf\n",
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00004-of-00005.gguf\n",
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf\n",
            "/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00003-of-00005.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73990688"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the GGUF model has been successfully split into multiple files, I will upload these files to the specified Hugging Face repository. I will use the `HfApi` from `huggingface_hub` to upload each file found by the `glob` pattern.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "bf755fd20dc54a0b997cba211c43d9c4",
            "f726f6c32be549cb86fd64c6466b11b6",
            "053237f16c2e4d02ba5dab915363c35c",
            "8534e65382c1479c940e193dbac6cbd5",
            "123467e4a0eb4cfbbd69a05d9b559f38",
            "7ce2b0965ba54da8b89b95690930f3c4",
            "5dbaf5e8103a4e6f92561ba3dc54a379",
            "63dd0f89afb24ee5a99998c760b31d4d",
            "3198372e307e4858af10bbefcaed6535",
            "4e54d146f55942d9b87786b79d20b426",
            "5e7cd4da52b5431799226d1bc6ef691f",
            "0d44d349e5944dfea0efde21b37d570c",
            "67dd43c7bbc9480896b7f16dba3aa821",
            "69883353591e4f1eaf93cc4ee18f75e7",
            "9964199b3f0849d18aa7ac6c618b6fd4",
            "ff18a8cd5b524cb3b0465f316c35da1b",
            "a2ff9d8a233f454aa7025e2fcf12be8a",
            "9ad33ad5d42c447db3bfbe440fcc8bc9",
            "79a03a25b21448c59d15580470b16587",
            "2b5ba094ff7343c39fcc3469123724e7",
            "b2814a8a96a84cfab3f67fccbe1ce30d",
            "e10e618770c3448882219f9c19562798",
            "8b5caac462b1465b955e9930fcb2cde8",
            "6da21ae616284089ba00ddfc7137e6c6",
            "973fa4c079a94956ba63ef8f0647e532",
            "774821e4f39f49de87ad00ed2de514f6",
            "f893b778963245859677db1ba9f7a4e4",
            "e9d5f14b367b4f5b83dc9e6a7e72856d",
            "583f8552c5d1438c9a4399f93ab3a15f",
            "0149ce7da7324d1ea2aa33cb06d40371",
            "bafa854af70b4eac8875fd5f7347a9eb",
            "cd4f50cc953245c39a50671c17303e8e",
            "b7011fc972be48cda54f5fde13303b28",
            "34517a8d72a24817a8ba7e6ff7366a43",
            "8e0b15cc660c4cfd8420d33b758c82fe",
            "09b71e0f71294aebb2ef919f547a8f6f",
            "7199a3d6e0d4468d975037ad8dc05204",
            "ba0d5eb00e074703a81d4fa69ae2dbe2",
            "25e9b89720b04688b03db506e9c93bf0",
            "2192e9d572d24cb2a380afdf7eda7aee",
            "83ab5fafc0284149a64b1647f6db96e3",
            "cb64ce4e87d4492a8c185fe7107d41e1",
            "ccf682808da14200b1d5a676543cd5b0",
            "c9ba2cc06fd34b1cb7cacf278b2c63fb",
            "4c81bcaeb6de4ef1aa63dc5808f29093",
            "841b686ed20c43368dac0aa4a337336f",
            "621c21d3555b47c4a0ef58c8d07a1d2e",
            "925939f346634b9ba8d0e6f511ae17b3",
            "d1b15a6bfb404f4e869ef615196b1ebc",
            "19f933805d33466f9dd231e907e9d96a",
            "0fd1551e7c814291ad9d5c46484efb42",
            "7f353f5221634981bc41b09535a55be6",
            "c3091d0eed6a457f9388b3a675534665",
            "f573474c1cf94dc8a7aedb76558ca960",
            "d0b6f6c6164f4b028b757326c23b7431",
            "dc1a6b2fb5c44962937dddd1246743dc",
            "158a4468874c44a5be2c46d0ead222a1",
            "5d3c547e67f3407a92b9f571ac58f597",
            "a02a9de4699748c9b990f1326825dba6",
            "20130f664722456fa9e9f071e98b8935",
            "428bf7f8c5c04dfcae9694efb234556b",
            "d8e3421412f34c36a351d901afad132e",
            "1a2a03946e9f4579a1d20567bfa45c03",
            "1a0231ddd4c74a90a3687604c5bbd888",
            "968389d134fb4dcabb17e4f877c24871",
            "66fb428992eb40c490c71c8369dfdcb4",
            "e13000fe887f46c5991b1eaaac9721d3",
            "6af7fdd65dba4fe4aeb95714de2be8b5",
            "9c7a65b8d4d448638164b9170f475cdd",
            "d29991a8fba64f869e1e503c38e819c0",
            "a7454a8b87084a8f8ca00cba4265266f",
            "e15f09eb730242d2a3d50c75b0f9997c",
            "ada2b1bbe6514ed9be9f52164bc8f8af",
            "e04778741b904aa78238bed63fc1ba94",
            "7a4499368c80439fbe5dd3c3ba95c66a",
            "baffb32f987a41a1b2f0be9e1142639d",
            "39569003ea374cf3a63ec0a307fc3353",
            "715e7acbe8f5424f974bf72dd0793949",
            "b74799833d574c03b64e3714f5c0f686",
            "e38a208aa9524c9380e347c04de3d5c1",
            "e301b2c3cd924b12bbccbd30ae99305f",
            "a81750458d1e480593524fd6b4da178f",
            "6cfc240fe8a44bffbc35d4e0fb83dae2",
            "0904c9c0ef7049128beb0014aa67df8d",
            "9de578f57efd4374a6f1dd2ef1619c8b",
            "e99cb38b4f35456fb7be61c90098c573",
            "018a0178e1844ad7aa533f9cdb83ae4f",
            "52e634c606c34601adc1b8f87430543f",
            "a1aadb0173364940b820d5ff4c3085a5",
            "ec47c410da754421a7b708fcd49348ea",
            "345091e5c4be4f718ccaeb5db18b2a1c",
            "4163b5e20cb04acfb3cd7ac973190811",
            "7733d6cb853e47448d7030dec29c1024",
            "a6adc8e660f54373b1dc992c2dce619b",
            "b233fbb94465404c91aae050011705f6",
            "b991d2f92dc34ffe8f4527fa98689a13",
            "315f009e50734529a416d9ea2eb9f20a",
            "4bb9c5b5c1e04b1c91fac69e672d3ded",
            "473f75edc86c44158407b6dd19784f3a",
            "956f76c9da114cc3854a0251236196a9",
            "f5efd97b237e4e46a8a51c2840037b2e",
            "1761ad0289934d03aa0381360e0da410",
            "1e3888a0ffd04edd93c7b661852ab8c4",
            "8efbbedc27d243f5b028833901916fd6",
            "2142bb18a79c4557baa305323a069358",
            "316881cf388746179337229b647ee195",
            "2e6f19c6bf7f4e929db34fa1c3920737",
            "6af462f9a819493ca98fa496fada82f7",
            "b8104ddc812f49c08b5953e53598ad16",
            "1d8991458f004f1090ed06b257dec062",
            "51fd5df6f8b64e3abbae23dbebfd032d",
            "76286927d78940748cfa4ccecf727058",
            "197c3f0f88534939934a2e48c0a37fcb",
            "4949252e843f40368502154623ab3787",
            "f6a3e165bcea40a8bff19979639b07f5",
            "40a1c42573b04817864de6e4dadb9de8",
            "ede09a9302d74b428cce19c57f4d2702",
            "9c80719fe19940c08db480b0b66faeee",
            "f6e3362d27f24f60819273cf2e700b2b",
            "8ee143b56fbf4050b401f099feb12aa2",
            "7145fbb93da347d88068aad678a2827e",
            "ba3d47bc08104a17bbb0c22ee99f84b8",
            "4ec9b53ec019403eb224cf98129f213d",
            "d0f87af3c7cf4f699f8f91a6718ced64",
            "cb509e2a9dcb4001af2dbb095c0088ee",
            "5ab0422b48394319b9aafb7d5335ac39",
            "5a7b7275fa7c487786bd6b5a0f59be78",
            "137ed6d25ab8439997715a288f1313d9",
            "3d7374811a5b4a1cadc839650d3f33ae",
            "1a4a1de9f93d4270a1bacca3c53a92ee",
            "4b6a2da12d5c4e60825ec69dad602dce",
            "e50c3a9c12e34b3ebd4aa9c392bfd3ae",
            "1b4397883a9a4be6881e690f74351b31",
            "15ab066ec6684f61ad8b9fbdce9a4d50",
            "89106802ecb64f989d89e2a6c87ac599",
            "bcc1423084da48e58e00c9a8bb4528e8",
            "e50174376a36416795ed38444390ad91",
            "7749f7f732df47c2b26c72e5b705f120",
            "0786640f429442a1831ab70ec78518bc",
            "ac3da3a9a5ea4dd389bfeeb82b41bcb2",
            "b7219c61b68f4291ad3cca9f7e229d3d",
            "4d2381e2f7f542dca1e2c88cd73a90ce",
            "604501bbe0114d56a30bede6fcb8c133",
            "b48508d6ce7f4926ae91b19541e441f1",
            "02063dde9e764bee8684cb216491bd02",
            "b002eddfa4a24b77a8c4f83b700fcc99",
            "0a01a8a5ba254822a73aa0fcb891c072",
            "b0ca4495e4df4dd4b6a7ad23d579a2ba",
            "e0e2d0a75d4b473ba13d72520ded7e65",
            "a15e3f01172b4ea68b115193124b698e",
            "bff57f84dc92461eb38d47af47e1a5f5",
            "aaddb6c2923b4dbf9f3dd62eb4e8ae8d",
            "26525ebf5868444a9ae8e0a64b20ebb5",
            "137aaaac064240ec905888fcd4b61583",
            "cafe36645429424f8dcd1ee00dd16c35",
            "40f29dc8ee6346af835c7f1c4f928dce",
            "a13ba32a63214aaeb241b2f0b9235ffe",
            "df16554fb37d4b32b4506743444484c7",
            "3b0582ff44b74aea939d161e3387c453",
            "efdaf4e04327465c9b8474975824860f",
            "12c505dd30a44a319b58d5a337c3d084",
            "65a956aa65e84bf0a1b11665a4babffc",
            "16627e3dccc943f7a3642899020b2f9c",
            "e21acffe0f584ae3a81e47415684049d",
            "e23a0539db02456a8ed78d937950fc32"
          ]
        },
        "id": "aeb98c7d",
        "outputId": "e630e758-fa6c-48c7-f586-f8a2a72038b9"
      },
      "source": [
        "from huggingface_hub import HfApi\n",
        "import glob\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"asdgad/minimax\"\n",
        "file_pattern = \"/content/MiniMaxAI_MiniMax-M2.1-IQ1_S-*-of-*.gguf\" # Only target the split files\n",
        "split_files = glob.glob(file_pattern)\n",
        "\n",
        "if split_files:\n",
        "    print(f\"Uploading {len(split_files)} split GGUF files to {repo_id}...\")\n",
        "    for file_path in split_files:\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        print(f\"Uploading {file_name}...\")\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=file_name,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",\n",
        "        )\n",
        "    print(\"All split GGUF files uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"No split GGUF files found matching the pattern: {file_pattern}. Please ensure the splitting step was successful.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading 5 split GGUF files to asdgad/minimax...\n",
            "Uploading MiniMaxAI_MiniMax-M2.1-IQ1_S-00005-of-00005.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf755fd20dc54a0b997cba211c43d9c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d44d349e5944dfea0efde21b37d570c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...IQ1_S-00005-of-00005.gguf:   0%|          |  524kB / 7.14GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b5caac462b1465b955e9930fcb2cde8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading MiniMaxAI_MiniMax-M2.1-IQ1_S-00002-of-00005.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34517a8d72a24817a8ba7e6ff7366a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c81bcaeb6de4ef1aa63dc5808f29093"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...IQ1_S-00002-of-00005.gguf:   0%|          | 41.9MB / 9.95GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc1a6b2fb5c44962937dddd1246743dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e13000fe887f46c5991b1eaaac9721d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "715e7acbe8f5424f974bf72dd0793949"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...IQ1_S-00001-of-00005.gguf:   0%|          | 33.2MB / 9.99GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1aadb0173364940b820d5ff4c3085a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading MiniMaxAI_MiniMax-M2.1-IQ1_S-00004-of-00005.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "956f76c9da114cc3854a0251236196a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51fd5df6f8b64e3abbae23dbebfd032d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...IQ1_S-00004-of-00005.gguf:   0%|          | 16.7MB / 9.97GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba3d47bc08104a17bbb0c22ee99f84b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading MiniMaxAI_MiniMax-M2.1-IQ1_S-00003-of-00005.gguf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b4397883a9a4be6881e690f74351b31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b48508d6ce7f4926ae91b19541e441f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...IQ1_S-00003-of-00005.gguf:   0%|          | 16.7MB / 9.96GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cafe36645429424f8dcd1ee00dd16c35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All split GGUF files uploaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01227084"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that all split model files have been successfully uploaded to the Hugging Face repository and provide the link to the uploaded model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dfd6ce7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Confirmation of split model file upload:** All 5 split GGUF model files were successfully uploaded to the Hugging Face repository.\n",
        "*   **Link to the uploaded model:** The uploaded model files can be found at `https://huggingface.co/asdgad/minimax/tree/main`.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `huggingface_hub` library was confirmed to be already installed in the environment.\n",
        "*   Hugging Face authentication was successfully initiated, awaiting user token input.\n",
        "*   An initial check revealed that the GGUF model was not split, with only the original file present.\n",
        "*   The `llama-gguf-split` command was re-executed, successfully splitting the `/content/MiniMaxAI_MiniMax-M2.1-IQ1_S.gguf` model into 5 smaller GGUF files, each with a maximum size of 10GB.\n",
        "*   All 5 newly generated split GGUF files (e.g., `MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf`) were successfully uploaded to the Hugging Face repository \"asdgad/minimax\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Users can verify the uploaded split GGUF files directly on the Hugging Face model page.\n",
        "*   Ensure the integrity and functionality of the split model files on Hugging Face for future use or sharing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zNsumt2zoD9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-b7622/llama-cli -m /content/MiniMaxAI_MiniMax-M2.1-IQ1_S-00001-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLvB8VrWoEi6",
        "outputId": "1a93676f-63e5-40b5-82d9-93f4020e9a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/llama-b7622/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/llama-b7622/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OVgn5QWOoITD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/asdgad/minimax/tree/main"
      ],
      "metadata": {
        "id": "CoJwgC6UqjGt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KciGbB32qjmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}